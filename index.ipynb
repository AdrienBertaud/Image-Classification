{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "index.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "epfl_ann",
      "language": "python",
      "name": "epfl_ann"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnAEU9iKUmFl"
      },
      "source": [
        "# Miniproject: Image Classification\n",
        "\n",
        "### Description\n",
        "\n",
        "One of the oldest traditions in deep learning is to first tackle the fun problem of MNIST classification. [The MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) is a large database of handwritten digits that is commonly used as a first test for new classification algorithms. \n",
        "We follow this tradition to investigate the performance of artificial neural networks of different complexity on MNIST. However, since MNIST is too easy for accessing the full power of modern machine learning algorithms (see e.g. [this post](https://twitter.com/goodfellow_ian/status/852591106655043584)) we will extend our analysis to the recently introduced, harder [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist).\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/). Feel free to gain inspiration from the [Keras example directory](https://keras.io/examples/) for your implementations.\n",
        "- You should know the concepts \"multilayer perceptron\", \"stochastic gradient descent with minibatches\", \"Adam\", \"convolutional neural network\", \"training and validation data\", \"overfitting\", \"regularization\", and \"early stopping\".\n",
        "\n",
        "### What you will learn\n",
        "\n",
        "- You will learn how to define feedforward neural networks in keras and fit them to data (i.e. training).\n",
        "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
        "- You will gain some experience on the influence of network architecture, optimizer and regularization choices on the goodness of fit.\n",
        "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night (or on an external server).\n",
        "\n",
        "### Evaluation criteria\n",
        "\n",
        "The evaluation is (mostly) based on the figures you submit and your answer sentences. Provide clear and concise answers respecting the indicated maximum length.\n",
        "\n",
        "**The submitted notebook must be run by you!** We will only do random tests of your code and not re-run the full notebook. There will be fraud detection sessions at the end of the semester.\n",
        "\n",
        "### Your names\n",
        "\n",
        "**Before you start**: please enter your full name(s) in the field below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVa5G9-6UmFv"
      },
      "source": [
        "student1 = \"Jan Bauer (18-764-571)\"\n",
        "student2 = \"Adrien Bertaud (Sciper 324795)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4H4t1Ze2b09"
      },
      "source": [
        "using_mac_gpu = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZLXwbYkUmFy"
      },
      "source": [
        "## Some helper functions\n",
        "\n",
        "For your convenience we provide here some functions to preprocess the data and plot the results later. Simply run the following cells with `Shift-Enter`.\n",
        "\n",
        "### Dependencies and constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_-AKt04UmFz"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "if using_mac_gpu: \n",
        "    os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
        "\n",
        "import numpy as np\n",
        "from numpy.random import normal, choice\n",
        "\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import random\n",
        "\n",
        "#import tensorflow and keras\n",
        "import tensorflow as tf\n",
        "#import tensorflow.keras as keras\n",
        "#from tensorflow.keras import layers\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import Model\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, Input\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, History"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uByPWuGHUOb2"
      },
      "source": [
        "print(\"Running tensorflow version: {0}\".format(tf.__version__))\n",
        "print(\"Running keras version: {0}\".format(keras.__version__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzWXZqwSQtLp"
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx8b9KUvUprh"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eurfmdgSQtLr"
      },
      "source": [
        "tf.config.list_physical_devices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-U_Ag8QUmFz"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2V7in4BUmF0"
      },
      "source": [
        "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
        "                      ncols = 6, nrows = 4, xdim = 28, ydim = 28,\n",
        "                      label_mapping = range(10)):\n",
        "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
        "    \n",
        "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
        "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(nrows, ncols)\n",
        "    if len(select_from) == 0:\n",
        "        select_from = range(x.shape[0])\n",
        "    indices = choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
        "    for i, ind in enumerate(indices):\n",
        "        thisax = ax[i//ncols,i%ncols]\n",
        "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
        "        thisax.set_axis_off()\n",
        "        if len(y) != 0:\n",
        "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
        "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
        "                                                       verticalalignment='top',\n",
        "                                                       transform=thisax.transAxes)\n",
        "        if len(yhat) != 0:\n",
        "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
        "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
        "                                             verticalalignment='top',\n",
        "                                             horizontalalignment='right',\n",
        "                                             transform=thisax.transAxes)\n",
        "    return fig\n",
        "\n",
        "def prepare_standardplot(title, xlabel):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.suptitle(title)\n",
        "    ax1.set_ylabel('categorical cross entropy')\n",
        "    ax1.set_xlabel(xlabel)\n",
        "    ax1.set_yscale('log')\n",
        "    ax2.set_ylabel('accuracy [% correct]')\n",
        "    ax2.set_xlabel(xlabel)\n",
        "    return fig, ax1, ax2\n",
        "\n",
        "def finalize_standardplot(fig, ax1, ax2):\n",
        "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
        "    if len(ax1labels) > 0:\n",
        "        ax1.legend(ax1handles, ax1labels)\n",
        "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
        "    if len(ax2labels) > 0:\n",
        "        ax2.legend(ax2handles, ax2labels)\n",
        "    fig.tight_layout()\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "\n",
        "def plot_history(history, title):\n",
        "    return plot_history_custom(history.history, title)\n",
        "\n",
        "def plot_history_custom(history, title):\n",
        "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
        "    ax1.plot(history['loss'], label = \"train\")\n",
        "    ax1.plot(history['val_loss'], label = \"val\")\n",
        "    ax2.plot(history['accuracy'], label = \"train\")\n",
        "    ax2.plot(history['val_accuracy'], label = \"val\")\n",
        "    finalize_standardplot(fig, ax1, ax2)\n",
        "    return fig\n",
        "\n",
        "def plot_some_samples_custom(x, y = [], yhat = [], select_from = [], \n",
        "                      ncols = 6, nrows = 4, xdim = 28, ydim = 28,\n",
        "                      label_mapping = range(10)):\n",
        "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
        "    \n",
        "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
        "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(nrows, ncols)\n",
        "    if len(select_from) == 0:\n",
        "        select_from = range(x.shape[0])\n",
        "    indices = choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
        "    for i, ind in enumerate(indices):\n",
        "        thisax = ax[i//ncols,i%ncols]\n",
        "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
        "        thisax.set_axis_off()\n",
        "        if len(y) != 0:\n",
        "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
        "            thisax.text(0, 0, label_mapping[j], color='green', \n",
        "                                                       verticalalignment='top',\n",
        "                                                       transform=thisax.transAxes)\n",
        "        if len(yhat) != 0:\n",
        "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
        "            thisax.text(1, 0, label_mapping[k], color='red',\n",
        "                                             verticalalignment='top',\n",
        "                                             horizontalalignment='right',\n",
        "                                             transform=thisax.transAxes)\n",
        "    return fig\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96-3BgpfUmF1"
      },
      "source": [
        "## Exercise 1: Data import and visualization (6 points)\n",
        "\n",
        "The datasets we use in this project (MNIST, Fashion-MNIST) consists of grayscale images with 28x28 pixels. \n",
        "The low resolution (and grayscale) of the images certainly misses some information that could be helpful for classifying the images. However, since the data has lower dimensionality due to the low resolution, the training converges faster. This is an advantage in situations like here (or generally when prototyping), where we want to try many different things without having to wait too long. \n",
        "\n",
        "1. As a warm-up exercise, use the importer to (down-)load the MNIST and Fashion-MNIST dataset. Assign useful variables to test & train images and labels for both datasets respectively. (2 pts)\n",
        "2. Use the function `plot_some_samples` defined above to plot some samples of the two datasets. What do the green digits at the bottom left of each image indicate? (2 pts)\n",
        "3. To prepare for training: 1. transform the labels to one hot coding, i.e. for 5 classes, label 2 becomes the vector [0, 0, 1, 0, 0] (you can use `utils.to_categorical` function from keras), and 2. reshape (flatten) the input images to input vectors and rescale the input data into the range [0,1]. (2 pts)\n",
        "\n",
        "*Hint*: Keras comes with a convenient in-built [data importer](https://keras.io/datasets/) for common datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGiMLZFXUmF2"
      },
      "source": [
        "**Answer to Question 2** (1 sentence):<br/>\n",
        "The green digit at the bottom left corresponds to '(label_mapping[j]+1)%10' (*true class label incremented by one and divided modulo 10*) and can be interpreted as the index of the digit with value 1 (all others at 0) for one hot encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGSLbCcbUmF2"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44GJq_o2BlkA"
      },
      "source": [
        "#### Exercise 1.1\n",
        "Load the MNIST and Fashion-MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjga3NseUmF3"
      },
      "source": [
        "(x_mnist_train, y_mnist_train), (x_mnist_test, y_mnist_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "(x_mnist_fash_train, y_mnist_fash_train), (x_mnist_fash_test, y_mnist_fash_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK0e2jWyBxuY"
      },
      "source": [
        "#### Exercise 1.2\n",
        "Plot some samples of the two datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-tP-_yR_cIT"
      },
      "source": [
        "print(\"MNIST\")\n",
        "plot_some_samples(x_mnist_train, y_mnist_train, ncols=6, nrows=4);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM7sGUJ7CPXI"
      },
      "source": [
        "print(\"Fashion-MNIST\")\n",
        "plot_some_samples(x_mnist_fash_train, y_mnist_fash_train, ncols=6, nrows=4);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozrpz5-NB7wg"
      },
      "source": [
        "#### Exercise 1.3\n",
        "Prepare for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPw6fhwK2b1G"
      },
      "source": [
        "# AT EX 5.5 I CREATED A SHUFFLE FUNCTION THAT IS MORE SIMPLE\n",
        "\"\"\"\n",
        "def randomly_shuffle(x_data, y_data):\n",
        "    assert x_data.shape[0] == len(y_data)\n",
        "    indexes = np.arange(x_data.shape[0])\n",
        "    # shuffle 5 times!\n",
        "    for _ in range(5): indexes = np.random.permutation(indexes)  \n",
        "    x_data = x_data[indexes]\n",
        "    y_data = y_data[indexes]\n",
        "    return x_data, y_data\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIegP_lbkrcQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwBcC5vO2b1G"
      },
      "source": [
        "# IF YOU WANT TO SHUFFLE, THE OPTION EXIST IN KERAS FIT:\n",
        "\"\"\"\n",
        "fit(object, x = NULL, y = NULL, batch_size = NULL, epochs = 10,\n",
        "  verbose = getOption(\"keras.fit_verbose\", default = 1),\n",
        "  callbacks = NULL, view_metrics = getOption(\"keras.view_metrics\",\n",
        "  default = \"auto\"), validation_split = 0, validation_data = NULL,\n",
        "  shuffle = TRUE, class_weight = NULL, sample_weight = NULL,\n",
        "  initial_epoch = 0, steps_per_epoch = NULL, validation_steps = NULL,\n",
        "  ...)\n",
        "\n",
        "Arguments\n",
        "\n",
        "shuffle: \n",
        "Logical (whether to shuffle the training data before each epoch) or string (for \"batch\").\n",
        "\"\"\"\n",
        "#x_mnist_train, y_mnist_train = randomly_shuffle(x_mnist_train, y_mnist_train)\n",
        "#x_mnist_test, y_mnist_test = randomly_shuffle(x_mnist_test, y_mnist_test)\n",
        "\n",
        "#x_mnist_fash_train, y_mnist_fash_train = randomly_shuffle(x_mnist_fash_train, y_mnist_fash_train)\n",
        "#x_mnist_fash_test, y_mnist_fash_test = randomly_shuffle(x_mnist_fash_test, y_mnist_fash_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Q_Rl6c-DV87"
      },
      "source": [
        "# transform the labels to one hot coding\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# One hot encode the labels\n",
        "y_mnist_train_1hot = to_categorical(y_mnist_train)\n",
        "y_mnist_test_1hot = to_categorical(y_mnist_test)\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"train : {0} => {1} \\ttest : {2} => {3}\".format(y_mnist_train[i], y_mnist_train_1hot[i], y_mnist_test[i], y_mnist_test_1hot[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09HL6DMLN8uu"
      },
      "source": [
        "# One hot encode the labels\n",
        "y_mnist_fash_train_1hot = to_categorical(y_mnist_fash_train)\n",
        "y_mnist_fash_test_1hot = to_categorical(y_mnist_fash_test)\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"train : {0} => {1} \\ttest : {2} => {3}\".format(y_mnist_fash_train[i], y_mnist_fash_train_1hot[i], y_mnist_fash_test[i], y_mnist_fash_test_1hot[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tDl4RApFFV2"
      },
      "source": [
        "# MNIST: rescale the input data into the range [0,1]\n",
        "max_grey_value = 255.0\n",
        "\n",
        "print(\"MNIST\")\n",
        "print(\"Max value in initial dataset (train/test):\", np.max(x_mnist_train), np.max(x_mnist_test))\n",
        "print(\"Min value in initial dataset (train/test):\", np.min(x_mnist_train), np.min(x_mnist_test))\n",
        "x_mnist_train = x_mnist_train / max_grey_value  \n",
        "x_mnist_test = x_mnist_test / max_grey_value \n",
        "print(\"Max value in rescaled dataset (train/test):\", np.max(x_mnist_train), np.max(x_mnist_test))\n",
        "print(\"Min value in rescaled dataset (train/test):\", np.min(x_mnist_train), np.min(x_mnist_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xRmWrSuLyy5"
      },
      "source": [
        "# MNIST: rescale the input data into the range [0,1]\n",
        "print(\"Fashion-MNIST\")\n",
        "print(\"Max value in initial dataset (train/test):\", np.max(x_mnist_fash_train), np.max(x_mnist_fash_test))\n",
        "print(\"Min value in initial dataset (train/test):\", np.min(x_mnist_fash_train), np.min(x_mnist_fash_test))\n",
        "x_mnist_fash_train = x_mnist_fash_train / max_grey_value  \n",
        "x_mnist_fash_test = x_mnist_fash_test / max_grey_value \n",
        "print(\"Max value in rescaled dataset (train/test):\", np.max(x_mnist_fash_train), np.max(x_mnist_fash_test))\n",
        "print(\"Min value in rescaled dataset (train/test):\", np.min(x_mnist_fash_train), np.min(x_mnist_fash_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhdHjnvN2b1I"
      },
      "source": [
        "def cross_validation_split(x_data, y_data, val_count):\n",
        "    x_val = x_data[:val_count]\n",
        "    y_val = y_data[:val_count]\n",
        "    x_test = x_data[val_count:]\n",
        "    y_test = y_data[val_count:]\n",
        "    return x_val, y_val, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCS7n3_V2b1I"
      },
      "source": [
        "# Split test data into cross-val & test sets (use 8000 records in cross-val set and 2000 in the test set)  \n",
        "x_mnist_val, y_mnist_val_1hot, x_mnist_test, y_mnist_test_1hot = cross_validation_split(x_mnist_test, y_mnist_test_1hot, val_count=8000)\n",
        "x_mnist_fash_val, y_mnist_fash_val_1hot, x_mnist_fash_test, y_mnist_fash_test_1hot = cross_validation_split(x_mnist_fash_test, y_mnist_fash_test_1hot, val_count=8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whHrNeQiETo7"
      },
      "source": [
        "# reshape (flatten) the input images to input vectors\n",
        "print(\"MNIST\")\n",
        "original_mnist_train_shape = x_mnist_train.shape\n",
        "original_mnist_val_shape = x_mnist_val.shape\n",
        "original_mnist_test_shape = x_mnist_test.shape\n",
        "x_mnist_tr_flat = x_mnist_train.reshape(x_mnist_train.shape[0], x_mnist_train.shape[1]*x_mnist_train.shape[2])\n",
        "x_mnist_val_flat = x_mnist_val.reshape(x_mnist_val.shape[0], x_mnist_val.shape[1]*x_mnist_val.shape[2])\n",
        "x_mnist_te_flat = x_mnist_test.reshape(x_mnist_test.shape[0], x_mnist_test.shape[1]*x_mnist_test.shape[2])\n",
        "print(\"Reshaped training data from {0} to {1}\".format(original_mnist_train_shape, x_mnist_tr_flat.shape))\n",
        "print(\"Reshaped validation data from {0} to {1}\".format(original_mnist_val_shape, x_mnist_val_flat.shape))\n",
        "print(\"Reshaped test data from {0} to {1}\".format(original_mnist_test_shape, x_mnist_te_flat.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evnIslVWHHmz"
      },
      "source": [
        "print(\"Fashion-MNIST\")\n",
        "original_train_shape = x_mnist_fash_train.shape\n",
        "original_val_shape = x_mnist_fash_val.shape\n",
        "original_test_shape = x_mnist_fash_test.shape\n",
        "x_fashion_tr_flat = x_mnist_fash_train.reshape(x_mnist_fash_train.shape[0], x_mnist_fash_train.shape[1]*x_mnist_fash_train.shape[2])\n",
        "x_fashion_val_flat = x_mnist_fash_val.reshape(x_mnist_fash_val.shape[0], x_mnist_fash_val.shape[1]*x_mnist_fash_val.shape[2])\n",
        "x_fashion_te_flat = x_mnist_fash_test.reshape(x_mnist_fash_test.shape[0], x_mnist_fash_test.shape[1]*x_mnist_fash_test.shape[2])\n",
        "print(\"Reshaped training data from {0} to {1}\".format(original_train_shape, x_fashion_tr_flat.shape))\n",
        "print(\"Reshaped validation data from {0} to {1}\".format(original_val_shape, x_fashion_val_flat.shape))\n",
        "print(\"Reshaped test data from {0} to {1}\".format(original_test_shape, x_fashion_te_flat.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cZuTQVecZJB"
      },
      "source": [
        "# Constants\n",
        "input_dim_mnist = x_mnist_tr_flat.shape[1]\n",
        "input_dim_fashion = x_fashion_tr_flat.shape[1]\n",
        "# Get number of classes\n",
        "mnist_classes_nb = y_mnist_train_1hot.shape[1]\n",
        "fashion_classes_nb = y_mnist_fash_train_1hot.shape[1]\n",
        "print(\"MMNIST Input Shape {0}; MNIST Number Of Classes {1}\".format(input_dim_mnist, mnist_classes_nb))\n",
        "print(\"MMNIST Fashion Input Shape {0}; MNIST Fashion Number Of Classes {1}\".format(input_dim_fashion, fashion_classes_nb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nWca9BqUmF3"
      },
      "source": [
        "## Exercise 2: No hidden layer (10 points)\n",
        "\n",
        "Define and fit a model without a hidden layer (since we will use multi-layer models later in this project, you can define a general constructor function for models with an arbitrary number of hidden layers already at this point). Note that in this miniproject there is no real test dataset and what is loaded as a test dataset is used for validation. First, implement 1.-3. for the MNIST dataset.  \n",
        "\n",
        "1. Implement the model with the following specifications: use the softmax activation for the output layer, use the categorical_crossentropy loss, choose stochastic gradient descent for the optimizer, and add the accuracy metric to the metrics. (5 pts)\n",
        "2. Train for as many epochs as needed to see no further decrease in the validation loss. (1 pt)\n",
        "3. Plot the learning curves resulting from the fitting procedure (a history object) using the function `plot_history` defined above. (1 pt)\n",
        "4. Repeat the above steps for fitting the network to the Fashion-MNIST dataset. (2 pts)\n",
        "5. Report the best validation accuracy achieved for each one of the datasets. Do you observe overfitting already for this simple model? Answer separetely for the MNIST dataset and Fashion-MNIST dataset. (1 pt) \n",
        "\n",
        "*Hint:* Read the keras docs, in particular [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0om4_uXUmF5"
      },
      "source": [
        "**Answer to Question 5** (max 2 sentences):<br/>\n",
        "*MNIST:*\n",
        "\n",
        "We see that the improvement of the validation accuracy levels out around 20 epochs.\n",
        "\n",
        "If overfitting, we should see the validation loss stopping to decrease (i.e. increasing). Furthermore, since there is a lot of training data for a models with only 7960 parameters it is unlikely that the model is overfitting.\n",
        "\n",
        "*Fashion-MNIST:*\n",
        "\n",
        "We see that the improvement of the validation accuracy levels out around after 75 epochs\n",
        "\n",
        "Same interpretation than MNIST regarding overfitting, even more Fashion-MNIST has been introduced as beeing \"harder\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgSYz8gAUmF5"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wk7rbJWAtQL"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWptcE9GAuRD"
      },
      "source": [
        "def get_save_dir():\n",
        "  \"\"\"Get directory where models and histories are saved.\"\"\"\n",
        "  save_dir = 'save'\n",
        "  if os.path.isdir(save_dir) is not True:\n",
        "    os.mkdir(save_dir)\n",
        "  return save_dir\n",
        "\n",
        "\n",
        "def get_path_history(exercise_name):\n",
        "  \"\"\"Get path of history file.\"\"\"\n",
        "  return os.path.join(get_save_dir(), exercise_name + '_history.npy')\n",
        "\n",
        "\n",
        "def get_path_model(exercise_name):\n",
        "  \"\"\"Get path of model file.\"\"\"\n",
        "  return os.path.join(get_save_dir(), exercise_name + '_model.keras')\n",
        "\n",
        "\n",
        "def get_callbacks(early_stop = True, save_model=False, exercise_name=''):\n",
        "  callbacks = []\n",
        "\n",
        "  if save_model:\n",
        "    save_callback = ModelCheckpoint(get_path_model(exercise_name), monitor='val_loss', mode='min', save_best_only=True, verbose=True)\n",
        "    callbacks.append(save_callback)\n",
        "\n",
        "  if early_stop:\n",
        "    stop_callback = EarlyStopping(monitor='val_loss', mode='min', verbose=True, patience=20)\n",
        "    callbacks.append(stop_callback) \n",
        "\n",
        "  return callbacks  \n",
        "\n",
        "def fit_custom(exercise_name, model, x, y, validation_data=None, epochs=None, callbacks=None, verbose=False):\n",
        "  \"\"\"\n",
        "  Load saved history or launch fit if it does not exist.\n",
        "\n",
        "  INPUTS:\n",
        "  - exercise_name: string name of the exercise, it is used to store the history and reload it\n",
        "  - model: Keras Model to apply fit to. \n",
        "  \n",
        "  Others inputs are named in the same way that the Keras fit function, and are directly passed to it:\n",
        "  - x : Vector, matrix, or array of training data\n",
        "  - y : Vector, matrix, or array of target (label) data\n",
        "  - validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch.\n",
        "  - epochs : Number of epochs to train the model.\n",
        "  - verbose : Verbosity mode (0 = silent, 1 = progress bar, 2 = one line per epoch)\n",
        "  - callbacks : List of callbacks to be called during training.\n",
        "  \"\"\"\n",
        "  path_history = get_path_history(exercise_name)\n",
        "\n",
        "  if not os.path.isfile(path_history):\n",
        "    print(\"Begin to train the model {0}.\".format(exercise_name))\n",
        "    fit_output = model.fit(x=x, y=y, validation_data=validation_data, epochs=epochs, callbacks=callbacks, verbose=verbose)\n",
        "    print(\"Finished training the model {0}.\".format(exercise_name))\n",
        "    history = fit_output.history\n",
        "    np.save(path_history, history)\n",
        "  else:\n",
        "    print(\"Loading\", path_history)\n",
        "    history = np.load(path_history,allow_pickle='TRUE').item()\n",
        "\n",
        "  return history\n",
        "\n",
        "\n",
        "def fit_custom_(exercise_name, model, x_train, y_train, x_val, y_val, epochs=None, callbacks=None, verbose=False):\n",
        "  \"\"\"Same than fit_custom, but with another signature for convenience.\"\"\"\n",
        "  return fit_custom(\n",
        "      exercise_name=exercise_name, \n",
        "      model=model,\n",
        "      x=x_train, \n",
        "      y=y_train,\n",
        "      validation_data=(x_val, y_val),       \n",
        "      epochs=epochs, \n",
        "      callbacks=callbacks, \n",
        "      verbose=verbose)\n",
        "\n",
        "def get_model(input_dim, layers, h, optimizer, lr, dropout = 0, l2 = 0):\n",
        "    \"\"\"\n",
        "    Return a compiled model with:\n",
        "    - 'input_dim' input dimension \n",
        "    - 'layers' hidden layers \n",
        "    - 'h' units in each hidden layer\n",
        "    - 'optimizer' optimizer\n",
        "    - \"lr' learning rate\n",
        "    - 'dropout' dropout rate\n",
        "    - 'l2' L2 regularization factor\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(mnist_classes_nb, activation='relu', name=\"input_layer\",  input_shape=(input_dim_mnist,)))\n",
        "\n",
        "    if(dropout > 0):\n",
        "      model.add(Dropout(dropout))\n",
        "\n",
        "    for i in range(layers):\n",
        "      kernel_regularizer = None\n",
        "\n",
        "      if(l2>0):\n",
        "        kernel_regularizer = regularizers.l1_l2(l1=0, l2=l2)\n",
        "\n",
        "      model.add(Dense(h, activation='relu', kernel_regularizer=kernel_regularizer))\n",
        "\n",
        "      if(dropout > 0):\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(mnist_classes_nb, activation='softmax', name=\"output_layer\"))\n",
        "    model.compile(optimizer=optimizer(lr), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7vGd8ZORprP"
      },
      "source": [
        "#### Exercise 2.1\n",
        "Implement the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS_KeLUAQWfS"
      },
      "source": [
        "model=get_model(input_dim=input_dim_mnist, layers=0, h=0, optimizer=SGD, lr=0.01)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDsqMm-ETZqT"
      },
      "source": [
        "#### Exercise 2.2\n",
        "Train for as many epochs as needed to see no further decrease in the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSsbkS50CGMe"
      },
      "source": [
        "exercise_name='ex_2.2'\n",
        "\n",
        "history = fit_custom(\n",
        "    exercise_name=exercise_name,\n",
        "    model=model, \n",
        "    x=x_mnist_tr_flat, \n",
        "    y=y_mnist_train_1hot, \n",
        "    validation_data=(x_mnist_val_flat, y_mnist_val_1hot), \n",
        "    epochs=100,\n",
        "    callbacks=get_callbacks(early_stop = True, save_model=False, exercise_name=exercise_name),\n",
        "    verbose=1\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6jIRPNeLR9X"
      },
      "source": [
        "#### Exercise 2.3\n",
        "\n",
        "Plot the learning curves resulting from the fitting procedure (a history object) using the function `plot_history` defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVP8kpKlJgmi"
      },
      "source": [
        "plot_history_custom(history, \"MNIST learning with no Hidden Layer\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jANnS4HqMROF"
      },
      "source": [
        "#### Exercise 2.4\n",
        "\n",
        "Repeat with the Fashion-MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVur_FZVUacH"
      },
      "source": [
        "model=get_model(input_dim=input_dim_fashion, layers=0, h=0, optimizer=SGD, lr=0.01)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeS99WhhMW_-"
      },
      "source": [
        "exercise_name = 'ex_2.4'\n",
        "history = fit_custom(\n",
        "    exercise_name=exercise_name, \n",
        "    model=model,\n",
        "    x=x_fashion_tr_flat, \n",
        "    y=y_mnist_fash_train_1hot, \n",
        "    validation_data=(x_fashion_te_flat, y_mnist_fash_test_1hot), \n",
        "    epochs=100,\n",
        "    callbacks=get_callbacks(early_stop = True, save_model=False, exercise_name=exercise_name),\n",
        "    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYnfImWfNT9o"
      },
      "source": [
        "plot_history_custom(history, \"Fashion-MNIST learning with no hidden layer \");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvUjwawMNfS9"
      },
      "source": [
        "We see that the improvement of the validation accuracy levels out around 75 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93JrKMECUmF5"
      },
      "source": [
        "## Exercise 3: One hidden layer, different optimizers (10 points)\n",
        "\n",
        "Train a network with one hidden layer and compare different optimizers for the MNIST dataset.\n",
        "\n",
        "1. Use one hidden layer with 128 units and the 'relu' activation. Use the [summary method](https://keras.io/api/models/model/#summary-method) to display your model in a compact way. (1 pt)\n",
        "2. Train the model for at least 50 epochs with different learning rates of stochastic gradient descent (SGD). What happens if the learning rate $\\eta$ is very large (for ex. $\\eta=1$), and very small (for ex. $\\eta=0.001$)? Comment on the fluctuations of the learning curve. (2 pts)\n",
        "3. Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam) (you can use the default learning rate). (1pt)\n",
        "4. Plot the learning curves of SGD with a good learning rate (i.e. in the range [0.01,0.1]) together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot. (1 pts)\n",
        "5. Explain the qualitative difference between the loss and accuracy curves with respect to signs of overfitting. Report the best validation accuracy achieved for SGD and Adam. Which one is better and why do you think so? (2 pts)\n",
        "6. Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function \n",
        "   `plot_some_samples`. (1 pt)\n",
        "\n",
        "Real-world datasets are labeled by some people and sometimes there are mistakes in the labeling. We will corrupt labels of the MNIST dataset artifically, and observe an overfitting to this noisy dataset with Adam. \n",
        "\n",
        "7. Take $ p = 0.2 $ fraction of the data points from the training dataset of MNIST and change their class labels randomly. (You can sample a random integer from 0 to 9 using `np.random.uniform` and `np.floor`). Train with Adam for 50 or 100 epochs. Plot the learning curves. Do you observe overfitting in the validation accuracy? Does it take longer to converge to perfect training accuracy compare to noise-free MNIST? (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqsjfxPdUmF5"
      },
      "source": [
        "**Answer to Question 2** (max 2 sentences): We can see in the diagrams that fluctuations of the learning curve are increasing with the learning rate (for small learning rates {0.01, 0.001} almost no fluctuations in the learning curve can be seen anymore and the shape of the learning curve is really smooth). This can be explained by the fact that with a small learning rates, the algorithm makes smaller steps, and so weights change less before correcting direction.\n",
        "\n",
        "**Answer to Question 5** (max 3 sentences): Adam is faster to learn but at the end they perform in the same way (so faster leaning, but similar performances). We can explain it, saying that the networks are the same, but in Adam case the learning rate is adapting. To get models of similar performances, we must stop learning when it begins to overfit (around 5 epochs for Adam).\n",
        "\n",
        "**Answer to Question 7** (max 2 sentences): The noisy dataset begins to overfit after around 5 epochs. The model is definetly overfitting to the noise in the noisy MNIST dataset. It takes much longer to converge towards the perfect training accuracy on the noisy MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqNAh2guUmF5"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzbbCPSPBje0"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8E46szjUOcC"
      },
      "source": [
        "def train_with_various_lr(exercise_name, model, learning_rates, optimizer):\n",
        "  \"\"\"Train with various learning rates.\"\"\"\n",
        "  histories = []\n",
        "\n",
        "  for lr in learning_rates:\n",
        "    name = exercise_name + '_lr_{0}'.format(lr)\n",
        "    history = fit_custom(\n",
        "          exercise_name=name, \n",
        "          model=model(lr), \n",
        "          x=x_mnist_tr_flat, \n",
        "          y=y_mnist_train_1hot, \n",
        "          validation_data=(x_mnist_te_flat, y_mnist_test_1hot), \n",
        "          epochs=50, \n",
        "          callbacks=get_callbacks(early_stop = True, save_model=True, exercise_name=name),\n",
        "          verbose=False)\n",
        "      \n",
        "    optimizer_name = str(optimizer).split(\".\")[-1].replace('>', '').replace(\"'\", \"\")\n",
        "\n",
        "    plot_history_custom(\n",
        "        history, \n",
        "        \"MNIST learning with {0} and learning rate {1}\".format(optimizer_name,lr));\n",
        "\n",
        "    histories.append(history)\n",
        "  return histories\n",
        "\n",
        "def comparison_plot_custom(history_1, history_2, label1, label2, title):\n",
        "  \"\"\"Plot to compare two histories.\"\"\"\n",
        "  fig, ax1, ax2 = prepare_standardplot(title, \"epochs\")\n",
        "  ax1.plot(history_1['loss'], label=label1 + ' train')\n",
        "  ax1.plot(history_1['val_loss'], label=label1 + ' val')\n",
        "  ax1.plot(history_2['loss'], label=label2 + ' train')\n",
        "  ax1.plot(history_2['val_loss'], label=label2 + ' val')\n",
        "  ax2.plot(history_1['accuracy'], label=label1 + ' train')\n",
        "  ax2.plot(history_1['val_accuracy'], label=label1 + ' val')\n",
        "  ax2.plot(history_2['accuracy'], label=label2 + ' train')\n",
        "  ax2.plot(history_2['val_accuracy'], label=label2 + ' val')\n",
        "  finalize_standardplot(fig, ax1, ax2)\n",
        "  return fig\n",
        "\n",
        "def noise_labels(y_hot, p):\n",
        "  \"\"\"Noise one hot labels according to the fraction 'p'.\"\"\"\n",
        "  length = y_hot.shape[0]\n",
        "  number_of_classes = y_hot.shape[1]\n",
        "\n",
        "  # Define indices to noise\n",
        "  indices = np.arange(length)\n",
        "  noise_number_mnist = int(length*p)\n",
        "  noisy_indices = choice(indices, noise_number_mnist)\n",
        "\n",
        "  # Generate noisy labels\n",
        "  y_1hot_noisy = y_hot.copy()\n",
        "  for index in noisy_indices:\n",
        "    class_index = np.floor(np.random.uniform(0,number_of_classes-0.01))\n",
        "    y_1hot_noisy[index] = to_categorical(class_index, num_classes=mnist_classes_nb)\n",
        "\n",
        "  return y_1hot_noisy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WnT_AcObgs"
      },
      "source": [
        "#### Exercise 3.1\n",
        "\n",
        "Use one hidden layer with 128 units and the 'relu' activation. Use the [summary method](https://keras.io/api/models/model/#summary-method) to display your model in a compact way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE8X5bTnUOcC"
      },
      "source": [
        "def get_model_ex_3_1(learning_rate):\n",
        "  \"\"\"Create a model with 1 hidden layer and 128 units, compiled with SGD optimizer.\"\"\"\n",
        "  return get_model(input_dim=input_dim_mnist, layers=1, h=128, optimizer=SGD, lr=learning_rate)\n",
        "\n",
        "get_model_ex_3_1(0.01).summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqZiIDyzdnWz"
      },
      "source": [
        "#### Exercise 3.2\n",
        "\n",
        "Train the model for at least 50 epochs with different learning rates of stochastic gradient descent (SGD). What happens if the learning rate  𝜂 is very large (for ex.  𝜂=1), and very small (for ex.  𝜂=0.001)? Comment on the fluctuations of the learning curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suEEtwKhEEwc"
      },
      "source": [
        "learning_rates = [10**(-e) for e in range(0,4)]\n",
        "\n",
        "histories_sgd = train_with_various_lr(\n",
        "    exercise_name='ex_3.2', \n",
        "    model=get_model_ex_3_1,\n",
        "    learning_rates=learning_rates, \n",
        "    optimizer=SGD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwlnJS6JFac4"
      },
      "source": [
        "#### Exercise 3.3\n",
        "\n",
        "Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam) (you can use the default learning rate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdGpkqGNYQGc"
      },
      "source": [
        "def get_model_ex_3_3(learning_rate):\n",
        "  \"\"\"Create a model with 1 hidden layer and 128 units, compiled with Adam optimizer.\"\"\"\n",
        "  return get_model(input_dim=input_dim_mnist, layers=1, h=128, optimizer=Adam, lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYnNbIbhFXQC"
      },
      "source": [
        "adam_lr = 0.001\n",
        "histories_adam = train_with_various_lr(\n",
        "    exercise_name='ex_3.3', \n",
        "    model = get_model_ex_3_3,\n",
        "    learning_rates=[adam_lr], \n",
        "    optimizer=Adam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4nlIXiGL58O"
      },
      "source": [
        "#### Exercise 3.4\n",
        "\n",
        "Plot the learning curves of SGD with a good learning rate (i.e. in the range [0.01,0.1]) together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJNmTC4fMl4i"
      },
      "source": [
        "sgd_lr = 0.1\n",
        "history_good_sgd = histories_sgd[learning_rates.index(sgd_lr)]\n",
        "comparison_plot_custom(\n",
        "    history_good_sgd, \n",
        "    histories_adam[0], \n",
        "    \"SGD (LR {0})\".format(sgd_lr), \n",
        "    \"Adam(LR {0})\".format(adam_lr), \n",
        "    \"Optimizer Comparison\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmqo7WOOUOcE"
      },
      "source": [
        "#### Exercise 3.5\n",
        "Explain the qualitative difference between the loss and accuracy curves with respect to signs of overfitting. Report the best validation accuracy achieved for SGD and Adam. Which one is better and why do you think so?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUVfy9PHH9eF"
      },
      "source": [
        "print(\"Best validation accuracies:\")\n",
        "print(\"- SGD: {0}.\".format(round(max(history_good_sgd[\"val_accuracy\"]), 2)))\n",
        "print(\"- Adam: {0}.\".format(round(max(histories_adam[0][\"val_accuracy\"]), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1pi3qoPUOcE"
      },
      "source": [
        "### Exercise 3.6\n",
        "Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function `plot_some_samples`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XkG6vbbeNx9"
      },
      "source": [
        "exercise_name = \"ex_3.2_lr_{0}\".format(str(sgd_lr))\n",
        "model_sgd = load_model(get_path_model(exercise_name), compile=True)\n",
        "err, acc = model_sgd.evaluate(x_mnist_te_flat, y_mnist_test_1hot, verbose=0)\n",
        "print(\"{name}: err: {err}, acc: {acc}\".format(name=exercise_name, err=err, acc=acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBQeEFJfeNV6"
      },
      "source": [
        "exercise_name = \"ex_3.3_lr_{0}\".format(str(adam_lr))\n",
        "model_adam = load_model(get_path_model(exercise_name), compile=True)\n",
        "err, acc = model_adam.evaluate(x_mnist_te_flat, y_mnist_test_1hot, verbose=0)\n",
        "print(\"{name}: err: {err}, acc: {acc}\".format(name=exercise_name, err=err, acc=acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBRa0vEuUOcF"
      },
      "source": [
        "models = [model_sgd, model_adam]\n",
        "for model in models:\n",
        "    y_pred = model.predict_on_batch(x_mnist_te_flat).argmax(axis=1)\n",
        "    indices = [i for i,v in enumerate(y_pred) if y_pred[i]!=y_mnist_test[i]]\n",
        "    x_wrongly_predicted = np.asarray([x_mnist_te_flat[i] for i in indices])\n",
        "    y_wrongly_predicted = np.asarray([y_pred[i] for i in indices])\n",
        "    y_true_prediction = np.asarray([y_mnist_test[i] for i in indices])\n",
        "    plot_some_samples_custom(x_wrongly_predicted, y_true_prediction, y_wrongly_predicted, ncols=6, nrows=4);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b33M0ko9yZD"
      },
      "source": [
        "#### Exercise 3.7\n",
        "\n",
        "7. Take $ p = 0.2 $ fraction of the data points from the training dataset of MNIST and change their class labels randomly. (You can sample a random integer from 0 to 9 using `np.random.uniform` and `np.floor`). Train with Adam for 50 or 100 epochs. Plot the learning curves. Do you observe overfitting in the validation accuracy? Does it take longer to converge to perfect training accuracy compare to noise-free MNIST? (2 pts)<br/>\n",
        "**=> The training accuracy is increasing, starting from 80% and reaching around 90% after 100 episodes. The validation accuracy on the other hand is decreasing from 95% to around 80% after 100 episodes. The model is definetly overfitting to the noise in the shifted MNIST dataset. It takes much longer to converge towards the perfect training accuracy on the noisy MNIST dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIx3HIYmCUX3"
      },
      "source": [
        "# Train with noisy labels\n",
        "y_mnist_train_1hot_noisy = noise_labels(y_mnist_train_1hot, p=0.2)\n",
        "history_adam_noisy = model.compile(optimizer=Adam(learning_rate=adam_lr), loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "history_adam_noisy = fit_custom('ex_3.7_lr_{0}'.format(adam_lr),\n",
        "                                model=model,\n",
        "                                x_mnist_tr_flat, \n",
        "                                y_mnist_train_1hot_noisy, \n",
        "                                validation_data=(x_mnist_te_flat, y_mnist_test_1hot),                                  \n",
        "                                epochs=100, \n",
        "                                callbacks=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixnEny9WCXn9"
      },
      "source": [
        "plot_history_custom(history_adam_noisy, \"Noisy MNIST learning with Adam (learning rate {0})\".format(adam_lr));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3YDfszKDSPN"
      },
      "source": [
        "comparison_plot_custom(histories_adam[0], history_adam_noisy, \"MNIST Adam(LR {0})\".format(sgd_lr), \"Noisy MNIST Adam(LR {0})\".format(adam_lr), \"Optimizer Comparison\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oENMCYod9yZD"
      },
      "source": [
        "## Exercise 4: Model performance as a function of number of hidden neurons (8 + 2 points)\n",
        "\n",
        "Since the MNIST dataset is almost perfectly solved already by a one-hidden-layer network in Exercise 3, we use the Fashion-MNIST dataset from now on to compare the performances of more complex models. In this exercise, we investigate how the best validation loss and accuracy depends on the number of hidden neurons in a single layer.\n",
        "\n",
        "1. Fit at least 4 models with different number of hidden neurons (i.e. width) between 10 and 1000 to the Fashion-MNIST dataset. Train with Adam for 50-100 epochs. (2 pts)\n",
        "2. Plot the best validation loss and accuracy versus the width. Is the observed trend in accordance with the [general approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)? Do you observe an overfitting due to the complexity of the wider networks with more parameters? Report the best validation accuracy. (2 pts)\n",
        "3. Choose $ p \\geq 0.5 $ fraction of the data points from the training dataset of Fashion-MNIST and change their class labels randomly as in Exercise 3. For this noisy Fashion-MNIST dataset, fit at least 4 models with different widths between 10 and 250. Train with Adam for at least 150 epochs. Plot the best validation loss and accuracy vs. width. (2 pts)\n",
        "4. BONUS: Add random Gaussian noise on the input pixels with mean 0 and variance between 0.01-0.5 and use the original labels. For this noisy Fashion-MNIST dataset, fit at least 4 models with different widths between 10 and 250. Train with Adam for at least 150 epochs. Plot the best validation loss and accuracy vs. width. (2 pts)\n",
        "5. Answer to the same questions in 2 for the noisy Fashion-MNIST dataset(s). Comment on the differences between width-performence curves for these two (or three) datasets. (2 pts)\n",
        "\n",
        "In this exercise we fit each model only for one initialization and random seed. In practice one would collect some statistics (e.g. 25-, 50-, 75-percentiles) for each layer size by fitting each model several times with different initializations and the random seeds. You may also want to do this here. It is a good exercise, but not mandatory as it takes quite a bit of computation time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcWkSkaj9yZE"
      },
      "source": [
        "**Answer to Question 2** (max 2 sentences):\n",
        "The Universal Approximation Theorem states that a neural network with one hidden layer and unlimited number of hidden neurons can approximate any continuous function for inputs within a specific range and this can also be observed with our example, since the best training accuracy is steadily increasing with the number of hidden neurons.\n",
        "We can see that our different models are overfitting and the overfitting increases with an increasing number of hidden neurons.\n",
        "\n",
        "**Answer to Question 5** (max 3 sentences): \n",
        "Although the training and validation accuracy of exercise 4.3's models is quite low (since the 50% of the training labels got changed), we can still see that the training accuracy rises significantly with the number of hidden neurons (From 12% to 26%). A neural network with many hidden nodes is still able to learn this highly dataset correctly and achieve a high training accuracy (in accordance with the universal approximation theorem).\n",
        "In exercise 4.4 we can see that fitting to this slightly noisy dataset still works pretty well, although the model is also overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMpbRWhR-TbE"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCB4v25AjunD"
      },
      "source": [
        "def adam_various_neurons(exercise_name, x, y, validation_data, numbers_of_hidden_neurons=[10, 50, 100, 500, 1000], epochs=50, lr=0.001):\n",
        "  \"\"\"Train with Adam with various number of hidden neurons.\"\"\"\n",
        "  histories=[]\n",
        "  metrics = {\"neurons\":[], \"min_loss\":[], \"min_val_loss\":[], \"max_accuracy\":[], \"max_val_accuracy\":[]}\n",
        "\n",
        "  for number_of_hidden_neurons in numbers_of_hidden_neurons:\n",
        "    model = get_model(input_dim=input_dim_fashion, layers=1, h=number_of_hidden_neurons, optimizer=Adam, lr=lr)\n",
        "    history = fit_custom(exercise_name + \"_ANN-with-{0}-hidden-neurons\".format(number_of_hidden_neurons),\n",
        "                         model=model, epochs=epochs,\n",
        "                         x=x, \n",
        "                         y=y, \n",
        "                         validation_data=validation_data,                         \n",
        "                         callbacks=get_callbacks(early_stop = False, save_model=False))\n",
        "    histories.append(history)\n",
        "    print(metrics)\n",
        "    metrics[\"neurons\"].append(number_of_hidden_neurons)\n",
        "    metrics[\"min_loss\"].append(min(history[\"loss\"]))\n",
        "    metrics[\"min_val_loss\"].append(min(history[\"val_loss\"]))\n",
        "    metrics[\"max_accuracy\"].append(max(history[\"accuracy\"]))\n",
        "    metrics[\"max_val_accuracy\"].append(max(history[\"val_accuracy\"]))\n",
        "\n",
        "  return histories, metrics\n",
        "\n",
        "def plot_metrics(metrics):\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "        fig.suptitle('Validation loss and accuracy / Number of hidden neurons')\n",
        "        fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.3, hspace=None)\n",
        "        fig.set_figheight(6)\n",
        "        fig.set_figwidth(12)\n",
        "        ax1.set_xscale('log')\n",
        "        ax1.set_ylabel(\"Validation loss\")\n",
        "        ax1.set_xlabel(\"number of hidden neurons\")\n",
        "        ax1.plot(metrics[\"neurons\"], metrics[\"min_loss\"])\n",
        "        ax1.plot(metrics[\"neurons\"], metrics[\"min_val_loss\"])\n",
        "        ax2.set_xscale('log')\n",
        "        ax2.set_ylabel(\"Validation accuracy\")\n",
        "        ax2.set_xlabel(\"number of hidden neurons\")\n",
        "        ax2.plot(metrics[\"neurons\"], metrics[\"max_accuracy\"]);\n",
        "        ax2.plot(metrics[\"neurons\"], metrics[\"max_val_accuracy\"]);\n",
        "        finalize_standardplot(fig, ax1, ax2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKMbJRAKUOcI"
      },
      "source": [
        "#### Exercise 4.1\n",
        "Fit at least 4 models with different number of hidden neurons (i.e. width) between 10 and 1000 to the Fashion-MNIST dataset. Train with Adam for 50-100 epochs. (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMLBhavDEQyI"
      },
      "source": [
        "_, metrics = adam_various_neurons(\n",
        "    exercise_name=\"ex_4.1\", \n",
        "    x=x_fashion_tr_flat,\n",
        "    y=y_mnist_fash_train_1hot, \n",
        "    validation_data=(x_fashion_te_flat, y_mnist_fash_test_1hot),\n",
        "    numbers_of_hidden_neurons=[10, 50, 100, 500, 1000],\n",
        "    epochs=50,\n",
        "    lr=adam_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HveKqgO3UZsT"
      },
      "source": [
        "#### Exercise 4.2\n",
        "Plot the best validation loss and accuracy versus the width. Is the observed trend in accordance with the general approximation theorem? Do you observe an overfitting due to the complexity of the wider networks with more parameters? Report the best validation accuracy. (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXQgJ7bsaKzp"
      },
      "source": [
        "plot_metrics(metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt-x74BbUOcJ"
      },
      "source": [
        "#### Exercise 4.3\n",
        "Choose $ p \\geq 0.5 $ fraction of the data points from the training dataset of Fashion-MNIST and change their class labels randomly as in Exercise 3. For this noisy Fashion-MNIST dataset, fit at least 4 models with different widths between 10 and 250. Train with Adam for at least 150 epochs. Plot the best validation loss and accuracy vs. width. (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgI2VKaItkWs"
      },
      "source": [
        "numbers_of_hidden_neurons=[10, 50, 100, 150, 200, 250]\n",
        "histories, metrics = adam_various_neurons(\n",
        "    exercise_name=\"ex_4.3\", \n",
        "    x=x_fashion_tr_flat, \n",
        "    y=noise_labels(y_mnist_fash_train_1hot, p=0.5), \n",
        "    validation_data=(x_fashion_te_flat, y_mnist_fash_test_1hot), \n",
        "    numbers_of_hidden_neurons=numbers_of_hidden_neurons,\n",
        "    epochs=150,\n",
        "    lr=adam_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDgRT_n-aTdU"
      },
      "source": [
        "plot_metrics(metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRQJHCh5UOcK"
      },
      "source": [
        "### Exercise 4.4\n",
        "Add random Gaussian noise on the input pixels with mean 0 and variance between 0.01-0.5 and use the original labels. For this noisy Fashion-MNIST dataset, fit at least 4 models with different widths between 10 and 250. Train with Adam for at least 150 epochs. Plot the best validation loss and accuracy vs. width. (2 pts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJCpUimg2SYu"
      },
      "source": [
        "_, metrics = adam_various_neurons(\n",
        "    exercise_name=\"ex_4.4\", \n",
        "    x=x_fashion_tr_flat + normal(loc=0,scale=0.05,size=(x_fashion_tr_flat.shape)),\n",
        "    y=y_mnist_fash_train_1hot, \n",
        "    validation_data=(x_fashion_te_flat, y_mnist_fash_test_1hot), \n",
        "    numbers_of_hidden_neurons=numbers_of_hidden_neurons,\n",
        "    epochs=150,\n",
        "    lr=adam_lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL1V8A_gF9Gl"
      },
      "source": [
        "for i in range(len(numbers_of_hidden_neurons)):\n",
        "  plot_history_custom(histories[i], \"{0} hidden neurons\".format(numbers_of_hidden_neurons[i]));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPzqei2OUOcL"
      },
      "source": [
        "plot_metrics(metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2Gb7mgSUmF7"
      },
      "source": [
        "## Exercise 5: Going deeper: tricks and regularization (10 + 2 points)\n",
        "\n",
        "Adding hidden layers to a deep network does not necessarily lead to a straight-forward improvement of performance. We use Fashion-MNIST dataset in this exercise.  \n",
        "\n",
        "1. Choose a width $ h $ between 50-200 and use at least two hidden layers with $ h $ hidden neurons (each) and train the network for at least 150 epochs. (2 pts)\n",
        "2. Plot the learning curves and report the best validation accuracy. Do you observe overfitting? (2 pts)\n",
        "3. Keep the network at the same size. Use a very large l2 regularization $\\lambda$ (for ex. $\\lambda=1$) and a small one (for ex. $\\lambda=0.001$) on kernel weights and report the validation accuracies. What do you observe?  (2 pts)\n",
        "4. Turn off the $ \\ell_2 $ regularization and use a large dropout rate (for ex. 0.5) and a small one (for ex. 0.05) at all hidden layers and report the validation accuracies. What do you observe? (2 pts)\n",
        "5. BONUS: Try built-in data augmentation methods as a way to regularize: this may include horizontal flipping of the images or small rotations. You can use built-in methods in Keras. Report the augmentation method you used and the best validation accuracy. (2 pts)\n",
        "\n",
        "Note that one needs to cross-validate to find the right regularization parameter for the model you chose and for the dataset at hand. However we do not enforce this hyperparameter search as it takes long computation times but it is a good practice if you try it here. \n",
        "\n",
        "6. Compare the validation accuracies resulting from your attempts to reduce overfitting. Did you improve the validation accuracy? If not, comment on the possible sources of failure. (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzX8ezheUmF7"
      },
      "source": [
        "**Answer to Question 2** (max 1 sentence):<br/>\n",
        "Overfitting can be observed in the training process since the loss is decreasing but the validation loss is increasing and the validation accuracy levels out after 25 epochs although the training accuracy still keeps increasing.\n",
        "\n",
        "Best validation accuracy: 0.8906999826431274\n",
        "\n",
        "**Answer to Question 3** (max 2 sentences): \n",
        "\n",
        "**Answer to Question 4** (max 2 sentences): \n",
        "\n",
        "**Answer to Question 5 (BONUS)** (max 2 sentences): \n",
        "\n",
        "**Answer to Question 6** (max 2 sentences): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3v2V_woX-r4"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0P5vrtGYmcO"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSQJfhZVUmF8"
      },
      "source": [
        "#### Exercise 5.1\n",
        "Choose a width $ h $ between 50-200 and use at least two hidden layers with $ h $ hidden neurons (each) and train the network for at least 150 epochs.\n",
        "#### Exercise 5.2\n",
        "Plot the learning curves and report the best validation accuracy. Do you observe overfitting?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhEOqRuMsQsb"
      },
      "source": [
        "h = 128\n",
        "history = fit_custom_(\n",
        "    exercise_name='ex_5.2_h_{0}'.format(h),\n",
        "    model=get_model(input_dim=input_dim_mnist, layers=2, h=h, optimizer=SGD, lr=0.01), \n",
        "    x_train=x_fashion_tr_flat, \n",
        "    y_train=y_mnist_fash_train_1hot, \n",
        "    x_val=x_fashion_val_flat,\n",
        "    y_val=y_mnist_fash_val_1hot, \n",
        "    epochs=150,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L99Grhrzw8T"
      },
      "source": [
        "plot_history_custom(history, \"Fashion MNIST learning with two hidden layers (h = {0})\".format(h));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNh71FOR0OlR"
      },
      "source": [
        "print(\"Best validation accuracy: {0}.\".format(round(max(history[\"val_accuracy\"]), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9nh4tRimYD8"
      },
      "source": [
        "#### Exercise 5.3\n",
        "Keep the network at the same size. Use a very large l2 regularization $\\lambda$ (for ex. $\\lambda=1$) and a small one (for ex. $\\lambda=0.001$) on kernel weights and report the validation accuracies. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zuVW_yj2VE0"
      },
      "source": [
        "l2 = 1\n",
        "history = fit_custom(exercise_name='ex_5.3.1_h_{0}_l2_{1}'.format(h, l2),\n",
        "                     model=get_model(input_dim=input_dim_mnist, layers=2, h=h, optimizer=SGD, lr=0.01, l2=l2), \n",
        "                     x_train=x_fashion_tr_flat,\n",
        "                     y_train=y_mnist_fash_train_1hot, \n",
        "                     x_val=x_fashion_val_flat,\n",
        "                     y_val=y_mnist_fash_val_1hot,                      \n",
        "                     epochs=150,\n",
        "                     verbose=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ_RLy0s3f-K"
      },
      "source": [
        "plot_history_custom(history, \"Fashion MNIST learning with two hidden layers (h = {0}, l2 = {1})\".format(h, l2));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_DO4dzj55vS"
      },
      "source": [
        "print(\"Best validation accuracy: {0}.\".format(round(max(history[\"val_accuracy\"]), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lSdCB306BFC"
      },
      "source": [
        "l2 = 0.001\n",
        "history = fit_custom(exercise_name='ex_5.3.2_h_{0}_l2_{1}'.format(h, l2),\n",
        "                     model=get_model(input_dim=input_dim_mnist, layers=2, h=h, optimizer=SGD, lr=0.01, l2=l2),\n",
        "                     x_train=x_fashion_tr_flat, \n",
        "                     y_train=y_mnist_fash_train_1hot, \n",
        "                     x_val=x_fashion_val_flat,\n",
        "                     y_val=y_mnist_fash_val_1hot,                      \n",
        "                     epochs=150,\n",
        "                     verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfwG9D7z6AhF"
      },
      "source": [
        "plot_history_custom(history, \"Fashion MNIST learning with two hidden layers (h = {0}, l2 = {1})\".format(h, l2));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y1Zw0oC9HWd"
      },
      "source": [
        "print(\"Best validation accuracy: {0}.\".format(round(max(history[\"val_accuracy\"]), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9httDcZInLhz"
      },
      "source": [
        "#### Exercise 5.4\n",
        "Turn off the $ \\ell_2 $ regularization and use a large dropout rate (for ex. 0.5) and a small one (for ex. 0.05) at all hidden layers and report the validation accuracies. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy7c047lEL39"
      },
      "source": [
        "dropout = 0.5\n",
        "history = fit_custom(exercise_name='ex_5.4_h_{0}_dropout_{1}'.format(h, dropout),\n",
        "                     x_train=x_fashion_tr_flat, \n",
        "                     y_train=y_mnist_fash_train_1hot, \n",
        "                     x_val=x_fashion_val_flat, \n",
        "                     y_val=y_mnist_fash_val_1hot, \n",
        "                     model=get_model(input_dim=input_dim_mnist, layers=2, h=h, optimizer=SGD, lr=0.01, dropout=dropout),\n",
        "                     epochs=150,\n",
        "                     verbose=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNYVEvleEnUU"
      },
      "source": [
        "plot_history_custom(history, \"Fashion MNIST learning with two hidden layers (h = {0}, dropout = {1})\".format(h, dropout));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL30QHhknfj3"
      },
      "source": [
        "print(\"Best validation accuracy: {0}.\".format(round(max(history[\"val_accuracy\"]), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsqM3VfdzBTP"
      },
      "source": [
        "#### Exercise 5.5\n",
        "\n",
        "BONUS: Try built-in data augmentation methods as a way to regularize: this may include horizontal flipping of the images or small rotations. You can use built-in methods in Keras. Report the augmentation method you used and the best validation accuracy. (2 pts) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGBvzM0vN2h7"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbQBoKsjceku"
      },
      "source": [
        "def data_augmentation(images, labels, batch_size):\n",
        "\n",
        "  data_gen_args = dict(featurewise_center=False,\n",
        "                    featurewise_std_normalization=False,\n",
        "                    rotation_range=15,\n",
        "                    width_shift_range=0.1,\n",
        "                    height_shift_range=0.1,\n",
        "                    horizontal_flip=True)\n",
        "  \n",
        "  # set depth at 1 \n",
        "  new_shape = (images.shape[0], images.shape[1], images.shape[2], 1)\n",
        "  images = np.reshape(images, new_shape)\n",
        "\n",
        "  image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "  image_datagen.fit(images, augment=True)\n",
        "  image_generator = image_datagen.flow(images, labels,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "  return image_generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQF_LuHZc3NU"
      },
      "source": [
        "# Use a small subset of the dataset to visualize the effect of data augmentation\n",
        "ncols=4\n",
        "nrows=2\n",
        "total = ncols * nrows\n",
        "x_mnist_fash_train_small = x_mnist_fash_train[0:total]\n",
        "y_mnist_fash_train_1hot_small = y_mnist_fash_train_1hot[0:total]\n",
        "\n",
        "image_generator_small = data_augmentation(x_mnist_fash_train_small, y_mnist_fash_train_1hot_small, batch_size=total)\n",
        "augmented_samples_small = next(image_generator_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miGSprpoYo_8"
      },
      "source": [
        "print(\"Initial images:\")\n",
        "plot_some_samples(x_mnist_fash_train_small, y_mnist_fash_train_1hot_small, ncols=ncols, nrows=nrows);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzr35KOCYd-1"
      },
      "source": [
        "print(\"Modified images:\")\n",
        "plot_some_samples(augmented_samples_small[0], augmented_samples_small[1], ncols=ncols, nrows=nrows);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeerLlT_cIG9"
      },
      "source": [
        "# Generate new data\n",
        "image_generator = data_augmentation(\n",
        "    x_mnist_fash_train, \n",
        "    y_mnist_fash_train_1hot, \n",
        "    batch_size=x_mnist_fash_train.shape[0])\n",
        "\n",
        "augmented_samples = next(image_generator)\n",
        "print(\"Generated dataset shape x: {0} / y : {1}\".format(augmented_samples[0].shape, augmented_samples[1].shape))\n",
        "\n",
        "# Flatten images\n",
        "x_fash_augmented = augmented_samples[0].reshape(x_mnist_fash_train.shape[0], x_mnist_fash_train.shape[1]*x_mnist_fash_train.shape[2])\n",
        "\n",
        "# Normalize\n",
        "x_fash_augmented = x_fash_augmented / max_grey_value\n",
        "\n",
        "y_fash_1hot_augmented = augmented_samples[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtiJ7m50aK4e"
      },
      "source": [
        "# Add to initial dataset\n",
        "x_fash_augmented = np.concatenate([x_fashion_tr_flat, x_fash_augmented])\n",
        "y_fash_1hot_augmented  = np.concatenate([y_mnist_fash_train_1hot, y_fash_1hot_augmented])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU0fUjlwehgb"
      },
      "source": [
        "def unison_shuffled_copies(a, b):\n",
        "    \"\"\"Shuffle both inputs along first axis in the same way.\"\"\"\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uzXg790sNlb"
      },
      "source": [
        "# Schuffle in the same way\n",
        "x_fash_augmented, y_fash_1hot_augmented = unison_shuffled_copies(x_fash_augmented, y_fash_1hot_augmented)\n",
        "\n",
        "print(\"Augmented x training data from {0} to {1}\".format(x_fashion_tr_flat.shape, x_fash_augmented.shape))\n",
        "print(\"Augmented y training data from {0} to {1}\".format(y_mnist_fash_train_1hot.shape, y_fash_1hot_augmented.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXmWoxAEWcTv"
      },
      "source": [
        "history = fit_custom(\n",
        "    exercise_name = \"ex_5.5\",\n",
        "    x=x_fash_augmented,\n",
        "    y=y_fash_1hot_augmented,\n",
        "    validation_data=(x_fashion_te_flat, y_mnist_fash_test_1hot), \n",
        "    model = get_model(input_dim=input_dim_fashion, layers=2, h=h, optimizer=SGD, lr=0.01),\n",
        "    epochs=100, \n",
        "    callbacks=get_callbacks(early_stop=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEdAkWYMlSs6"
      },
      "source": [
        "plot_history_custom(history=history, title='F-MNIST with data augmentation.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6K6G_nuzRAT"
      },
      "source": [
        "#### Exercise5.6. \n",
        "\n",
        "Compare the validation accuracies resulting from your attempts to reduce overfitting. Did you improve the validation accuracy? If not, comment on the possible sources of failure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDYfqS_MUmF9"
      },
      "source": [
        "## Exercise 6: Convolutional neural networks (CNNs) (10 points)\n",
        "\n",
        "Convolutional neural networks have an inductive bias that is well adapted to image classification. \n",
        "\n",
        "1. Design a convolutional neural network and train it without using explicit regularizers. (2 pts)\n",
        "2. Try to improve the best validation scores of the model by experiment with batch_normalization layers, dropout layers and l2-regularization on weights (kernels) and biases. (4 pts)\n",
        "3. After you have found good settings, plot the learning curves for both models, naive (=no tricks/regularization) and tuned (=tricks + regularized) together in a comparison plot. (2pts)\n",
        "4. How does the CNN performance compare to the so far best performing (deep) neural network model? (2 pts)\n",
        "\n",
        "*Hint:* You may get valuable inspiration from the keras [examples](https://keras.io/examples/), [for example](https://keras.io/examples/vision/mnist_convnet/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUbPDPV_UmF9"
      },
      "source": [
        "**Answer to Question 4**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-dXDw80UmF9"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4P8tBD5QtMZ"
      },
      "source": [
        "# Keras imports (from Tensorflow) - we are using Tensorflow's implementation of Keras\n",
        "#import keras.backend as K\n",
        "\n",
        "#from keras.models import Sequential\n",
        "#from keras.optimizers import Adam\n",
        "#from keras import regularizers\n",
        "# these are my helper functions for Keras\n",
        "\n",
        "def build_cnn(is_tuned=False):\n",
        "    \"\"\"\n",
        "    builds the base Keras model, without any regularization\n",
        "    \"\"\"\n",
        "    K.clear_session()\n",
        "    \n",
        "    l2_lambda = 0.010\n",
        "    l2 = None if not is_tuned else regularizers.l2(l2_lambda)\n",
        "\n",
        "    model = keras.Sequential()\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(28, 28, 1), kernel_regularizer=l2))\n",
        "    \n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.BatchNormalization())\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2))\n",
        "    \n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.BatchNormalization())\n",
        "        \n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.Dropout(0.125))\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=l2))\n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.BatchNormalization())\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2))\n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.BatchNormalization())\n",
        "   \n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2))) \n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.Dropout(0.25))\n",
        "        \n",
        "    model.add(keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=l2))\n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.BatchNormalization())\n",
        "    \n",
        "    model.add(keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2))\n",
        "    \n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.BatchNormalization())  \n",
        "    \n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2))) \n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.Dropout(0.5))\n",
        "    \n",
        "    model.add(keras.layers.Flatten()) \n",
        "    \n",
        "    model.add(keras.layers.Dense(1024, activation='relu', kernel_regularizer=l2))\n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "    model.add(keras.layers.Dense(512, activation='relu', kernel_regularizer=l2))\n",
        "    if is_tuned:\n",
        "        model.add(keras.layers.Dropout(0.5))\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "    \n",
        "    adam = Adam(lr=0.0001, decay=1e-6)\n",
        "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moEH1kje2b1j"
      },
      "source": [
        "# cross-train model & evaluate performance on train & cross-validation data\n",
        "def train_and_evaluate_model(exercise_name, model, X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "                             num_epochs=25, batch_size=32, \n",
        "                             callbacks=None, verbose=False):\n",
        "    \"\"\"\n",
        "    cross train & evaluate model's performance on training, cross-val & test datasets\n",
        "    @params:\n",
        "        - X_train/y_train: pre-processed training datasets\n",
        "        - X_val/y_val: pre-processed cross-validation datasets\n",
        "        - X_test/y_test: pre-processed testing datasets\n",
        "        - epochs (optional, default=25) - no of epochs for which training should be done\n",
        "        - batch_size (optional, default=32) - the batch size to use for mini-batch gradient descent training\n",
        "        - plot_title (optional, default=None) - plots loss vs epochs and accuracy vs epoch curves if assigned value\n",
        "            other than None. Uses this assigned value as the plot's title\n",
        "        - model_save_name (optional, default=None) - the file name to which the Keras model is to be saved to.\n",
        "    \"\"\"\n",
        "    path_history = get_path_history(exercise_name)\n",
        "    if not os.path.isfile(path_history):\n",
        "        # train on X_train/y_train & cross-validate on X_val/y_val\n",
        "        history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=callbacks, verbose=verbose)\n",
        "        np.save(path_history, history.history)\n",
        "        \n",
        "        # evaluate metrics\n",
        "        print(\"\\nEvaluating...\", flush=True)\n",
        "        print('Training data:', flush=True)\n",
        "        loss, acc = model.evaluate(X_train, y_train, verbose=1)\n",
        "        print(\"  Training : loss %.3f - acc %.3f\" % (loss, acc))\n",
        "        print('Cross-validation data:', flush=True)\n",
        "        loss, acc = model.evaluate(X_val, y_val, verbose=1)\n",
        "        print(\"  Cross-val: loss %.3f - acc %.3f\" % (loss, acc))\n",
        "        print('Test data:', flush=True)\n",
        "        loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "        print(\"  Testing  : loss %.3f - acc %.3f\" % (loss, acc))\n",
        "    else:\n",
        "        print(\"Loading\", path_history)\n",
        "        return np.load(path_history,allow_pickle='TRUE').item()\n",
        "    return history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvZz4x2WxLuS"
      },
      "source": [
        "#### Exercise 6.1.\n",
        "Design a convolutional neural network and train it without using explicit regularizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I23p0RZrQtMa"
      },
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "print(y_mnist_fash_train_1hot.shape)\n",
        "print(y_mnist_fash_val_1hot.shape)\n",
        "print(y_mnist_fash_test_1hot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHzcvHspQtMa"
      },
      "source": [
        "x_mnist_fash_train_re = x_mnist_fash_train.reshape(-1, 28, 28, 1)\n",
        "print(x_mnist_fash_train_re.shape)\n",
        "print(y_mnist_fash_train_1hot.shape)\n",
        "\n",
        "x_mnist_fash_val_re = x_mnist_fash_val.reshape(-1, 28, 28, 1)\n",
        "print(x_mnist_fash_val_re.shape)\n",
        "print(y_mnist_fash_val_1hot.shape)\n",
        "\n",
        "x_mnist_fash_test_re = x_mnist_fash_test.reshape(-1, 28, 28, 1)\n",
        "print(x_mnist_fash_test_re.shape)\n",
        "print(y_mnist_fash_test_1hot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Favsp81eQtMb"
      },
      "source": [
        "model_naive = build_cnn(is_tuned=False)\n",
        "model_naive.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jZ97kMzV0_E"
      },
      "source": [
        "name = \"ex_6.1\"\n",
        "callbacks = get_callbacks(early_stop = False, save_model=True, exercise_name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip7MP8o-QtMb"
      },
      "source": [
        "history_naive = train_and_evaluate_model(name, model_naive, x_mnist_fash_train_re, y_mnist_fash_train_1hot, x_mnist_fash_val_re, y_mnist_fash_val_1hot, x_mnist_fash_test_re, y_mnist_fash_test_1hot, num_epochs=25, batch_size=128, callbacks=callbacks, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rryS51A25q2j"
      },
      "source": [
        "plot_history_custom(history_naive, \"Training history of naive CNN\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D7tznxQVuyy"
      },
      "source": [
        "best_naive_model = load_model(get_path_model(name), compile=True)\n",
        "val_err_naive, val_acc_naive = best_naive_model.evaluate(x_mnist_fash_val_re, y_mnist_fash_val_1hot, verbose=0)\n",
        "test_err_naive, test_acc_naive = best_naive_model.evaluate(x_mnist_fash_test_re, y_mnist_fash_test_1hot, verbose=0)\n",
        "print(\"{name}: val_err: {err}, val_acc: {acc}\".format(name=name, err=val_err_naive, acc=val_acc_naive))\n",
        "print(\"{name}: test_err: {err}, test_acc: {acc}\".format(name=name, err=test_err_naive, acc=test_acc_naive))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsF75m_eQtMc"
      },
      "source": [
        "#### Exercise 6.2.\n",
        "Try to improve the best validation scores of the model by experiment with batch_normalization layers, dropout layers and l2-regularization on weights (kernels) and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ0FlhjK2b1l"
      },
      "source": [
        "model_tuned = build_cnn(is_tuned=True)\n",
        "model_tuned.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AJWHQitQtMd"
      },
      "source": [
        "name = \"ex_6.2\"\n",
        "callbacks = get_callbacks(early_stop = False, save_model=True, exercise_name=name)\n",
        "history_tuned = train_and_evaluate_model(\"ex_6.2\", model_tuned, x_mnist_fash_train_re, y_mnist_fash_train_1hot, x_mnist_fash_val_re, y_mnist_fash_val_1hot, x_mnist_fash_test_re, y_mnist_fash_test_1hot,\n",
        "                             num_epochs=25, batch_size=128, \n",
        "                             callbacks=callbacks, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H3dY-YV5ErB"
      },
      "source": [
        "plot_history_custom(history_tuned, \"Training history of tuned CNN\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrS2szdgUr6b"
      },
      "source": [
        "best_tuned_model = load_model(get_path_model(name), compile=True)\n",
        "val_err_tuned, val_acc_tuned = best_tuned_model.evaluate(x_mnist_fash_val_re, y_mnist_fash_val_1hot, verbose=0)\n",
        "test_err_tuned, test_acc_tuned = best_tuned_model.evaluate(x_mnist_fash_test_re, y_mnist_fash_test_1hot, verbose=0)\n",
        "print(\"{name}: val_err: {err}, val_acc: {acc}\".format(name=name, err=val_err_tuned, acc=val_acc_tuned))\n",
        "print(\"{name}: test_err: {err}, test_acc: {acc}\".format(name=name, err=test_err_tuned, acc=test_acc_tuned))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0v65dSCQtMf"
      },
      "source": [
        "#### Exercise 6.3.\n",
        "After you have found good settings, plot the learning curves for both models, naive (=no tricks/regularization) and tuned (=tricks + regularized) together in a comparison plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEO2MnMeQtMg"
      },
      "source": [
        "comparison_plot_custom(history_naive, history_tuned, \"Naive\", \"Tuned\", \"Model Comparison\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL7yWIo6QtMg"
      },
      "source": [
        "#### Exercise 6.4\n",
        "How does the CNN performance compare to the so far best performing (deep) neural network model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdHy7DFp4jJe"
      },
      "source": [
        "The best performing deep neural network achieves a validation accuracy of around 89%.\n",
        "However they tend to overfit to the training data.\n",
        "Especially the tuned CNN achieves much higher validation and test accuracy (~93%) with less overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrV1MA9t148x"
      },
      "source": [
        "print(\"{name}: val_err: {err}, val_acc: {acc}\".format(name=\"Naive CNN\", err=val_err_naive, acc=val_acc_naive))\n",
        "print(\"{name}: test_err: {err}, test_acc: {acc}\".format(name=\"Naive CNN\", err=test_err_naive, acc=test_acc_naive))\n",
        "print(\"---\")\n",
        "print(\"{name}: val_err: {err}, val_acc: {acc}\".format(name=\"Tuned CNN\", err=val_err_tuned, acc=val_acc_tuned))\n",
        "print(\"{name}: test_err: {err}, test_acc: {acc}\".format(name=\"Tuned CNN\", err=test_err_tuned, acc=test_acc_tuned))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}